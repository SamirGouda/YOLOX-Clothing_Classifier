seed: !!int 1

# define your model in models.py and set model param to class name ex. model: YourClass
# all your model args should be defined the same as your class and shoud be passed to model_params
model: !!python/name:model.PytorchLightningModule
model_params:
    backbone_model: !!python/name:torchvision.models.resnet50
    weights_dir: "pretrained_models/resnet50/resnet50_imagenet1k_v2.pth"
    output_dim: !!int 10 # 10 classes
    loss: !!python/name:torch.nn.CrossEntropyLoss
    loss_params:
        # weight: []
        reduction: "mean"
    # if you want to use Adam optimizer set optimizer: !!python/name:torch.optim.Adam
    optimizer: !!python/name:torch.optim.SGD
    optimizer_params:
        lr: !!float 0.04
        momentum: !!float 0.9
        weight_decay: !!float 0.0005
    lr_scheduler: !!python/name:torch.optim.lr_scheduler.StepLR
    lr_scheduler_params:
        step_size: 7
        gamma: 0.1


# pytorch lightning params
pl_params: 
    gradient_clip_val: !!float 2.0
    gradient_clip_algorithm: "norm"
    accumulate_grad_batches: !!int 2
    deterministic: !!null # set to true for reproducibility
    precision: 32
    auto_lr_find: True ##
    auto_scale_batch_size: binsearch # options = ['power', 'binsearch', None]

# define your dataset in datasets.py and set dataset param to class name 
# ex. dataset: !!python/name:datasets.YourDataset
# all your dataset args should be defined the same as your class and shoud be passed to dataset_params
dataset: !!python/name:dataset.ClothingSmallDataset

dataloader_params:
    batch_size: 32
    sampler: !!null # set to !!null if you don't want to use it

# checkpoint params
model_checkpoint_params:
    monitor: 'val_acc'
    mode: 'max'
    save_top_k: 3

# use automatic mixed precision
use_amp: False


# to enable stocastic weight averaging algorithm set use_swa to True
# swa_start is the epoch at which the lr_scheduler is dropped and swa_lr scheduler takes place
# anneal_epochs is the number of epochs (after swa takes place) where lr is changing
# anneal_strategy decides how the lr will change either linear or cosine function
# swa_lrs is a fixed learning rate where swa reaches after anneal_epochs
# see the link below for more visual guides about swa 
# https://stackoverflow.com/questions/68726290/setting-learning-rate-for-stochastic-weight-averaging-in-pytorch
use_swa: True
swa_params:
    swa_lrs: !!float 0.01
    swa_epoch_start: !!float 0.8 # the procedure will start from ``int(swa_epoch_start * max_epochs)`` epoch
    annealing_epochs: !!int 8
    annealing_strategy: !!str cos # options = ['linear', 'cos']